---
title: "Regresión"
author: "Martin Santamaria"
date: "1/12/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
Sys.setenv(RETICULATE_PYTHON = "/Python38")
library(reticulate)
```

### Regresión Lineal Simple

```{r}
sal = c(1.8,2.2,3.5,4.0,4.3,5.0)
tension = c(100,98,110,110,112,120)
media.sal = mean(sal)
media.tension = mean(tension)
var.sal = var(sal)
var.tension = var(tension)
cov.sal.tension = cov(sal,tension)
b1 = cov.sal.tension/var.sal
b0 = media.tension-b1*media.sal

lm(tension ~ sal)
```

### Comprobación

```{r}
(round(media.tension-b0-b1*media.sal,6))

tension.estimada = b0+b1*sal
(mean(tension.estimada)-mean(tension))

errores=tension.estimada-tension
SSE = sum(errores^2)
n=length(sal)
(estimacion.varianza = SSE/(n-2))
```

### Variabilidades

```{r}
SST = sum((tension-media.tension)^2)
SSR = sum((tension.estimada-media.tension)^2)
SSE = sum((tension-tension.estimada)^2)
round(SST-SSR-SSE,6)
R2=SSR/SST
R2=var(tension.estimada)/var(tension)

summary(lm(tension ~ sal))$r.squared
```

### Bondad de la regresión

```{r}
data(anscombe)
str(anscombe)

summary(lm(y1~x1,data=anscombe))$r.squared
summary(lm(y2~x2,data=anscombe))$r.squared
summary(lm(y3~x3,data=anscombe))$r.squared
summary(lm(y4~x4,data=anscombe))$r.squared
```

```{r}
par(mfrow=c(2,2))
plot(y1~x1,data=anscombe)
abline(lm(y1~x1,data=anscombe),col=2)
plot(y2~x2,data=anscombe)
abline(lm(y2~x2,data=anscombe),col=2)
plot(y3~x3,data=anscombe)
abline(lm(y3~x3,data=anscombe),col=2)
plot(y4~x4,data=anscombe)
abline(lm(y4~x4,data=anscombe),col=2)
```

#### Normalidad de los errores

```{r}
library(nortest)

lillie.test(errores)
```

### Estimadores de $\Beta_0$ y $\Beta_1$

```{r}
# b_1
alpha=0.05
S=sqrt(estimacion.varianza)
extremo.izquierda.b1 = b1-qt(1-alpha/2,n-2)*S/(sd(sal)*sqrt(n-1))
extremo.derecha.b1 = b1+qt(1-alpha/2,n-2)*S/(sd(sal)*sqrt(n-1))
print('Intervalo confianza para b1:')
print(c(extremo.izquierda.b1,extremo.derecha.b1))

# b_0
extremo.izquierda.b0 = b0-qt(1-alpha/2,n-2)*S*sqrt(1/n+media.sal^2/((n-1)*var(sal)))
extremo.derecha.b0 = b0+qt(1-alpha/2,n-2)*S*sqrt(1/n+media.sal^2/((n-1)*var(sal)))
print('Intervalo confianza para b0:')
print(c(extremo.izquierda.b0,extremo.derecha.b0))

confint(lm(tension~sal),level=0.95)
```

```{r}
alpha=0.05
x0=4.5
newdata=data.frame(sal=x0)

# Media poblacional dado un x0 concreto
y0.estimado = b0+b1*x0
extremo.izquierda.mu.x0 = y0.estimado-qt(1-alpha/2,n-2)*S*
  sqrt(1/n+(x0-media.sal)^2/((n-1)*var(sal)))
extremo.derecha.mu.x0 = y0.estimado+qt(1-alpha/2,n-2)*S*
  sqrt(1/n+(x0-media.sal)^2/((n-1)*var(sal)))
print(paste("Intervalo de confianza para mu de Y para x0=",x0))
print(c(extremo.izquierda.mu.x0,extremo.derecha.mu.x0))

predict.lm(lm(tension~sal),newdata,interval="confidence",level= 0.95)

# Para un valor (Persona) concreta
extremo.izquierda.y0 = y0.estimado-qt(1-alpha/2,n-2)*S*
  sqrt(1+1/n+(x0-media.sal)^2/((n-1)*var(sal)))
extremo.derecha.y0 = y0.estimado+qt(1-alpha/2,n-2)*S*
  sqrt(1+1/n+(x0-media.sal)^2/((n-1)*var(sal)))
print(paste("Intervalo de confianza para y0 para x0=",x0))
print(c(extremo.izquierda.y0,extremo.derecha.y0))

predict.lm(lm(tension~sal),newdata,interval="prediction",level= 0.95)
```

### Contrastes de Hipótesis

```{r}
t0 = b1/(S/(sd(sal)*sqrt(n-1)))
(p=2*pt(abs(t0),n-2,lower.tail = FALSE))

summary(lm(tension ~ sal))
```

### Regresión Lineal Múltiple

```{r}
X=matrix(c(1,78,48.2,2.75,29.5,1,69,45.5,2.15,26.3,
 1,77,46.3,4.41,32.2,1,88,49,5.52,36.5,
 1,67,43,3.21,27.2,1,80,48,4.32,27.7,
 1,74,48,2.31,28.3,1,94,53,4.3,30.3,
 1,102,58,3.71,28.7),nrow=9,byrow=TRUE)
y.bebes=c(57.5,52.8,61.3,67,53.5,62.7,56.2,68.5,69.2)
estimaciones.b = solve(t(X)%*%X)%*%(t(X)%*%y.bebes)

lm(y.bebes ~ X[,2]+X[,3]+X[,4]+X[,5])
```

```{r}
# La función de regresión pasa por el vector medio
vectores.medios = apply(X[,1:5],2,mean)
round(mean(y.bebes)-t(estimaciones.b)%*%vectores.medios,6)

# La media de los valores estimados se igual a la media de los observados
valores.estimados = X%*%estimaciones.b
round(mean(y.bebes)-mean(valores.estimados),6)

# Los errores tienen media 0 y varianza SSE/(n-1)
errores=y.bebes-valores.estimados
round(mean(errores))

SSE=sum(errores^2)
n=dim(X)[1]
var(errores)-SSE/(n-1)
```

### Variabilidades

```{r}
SST=sum((y.bebes-mean(y.bebes))^2)
SSR=sum((valores.estimados-mean(y.bebes))^2)
SSE = sum(errores^2)
round(SST-SSR-SSE,6)
R2=SSR/SST
R2 = var(valores.estimados)/var(y.bebes)

summary(lm(y.bebes~X[,2]+X[,3]+X[,4]+X[,5]))$r.squared
```

### $R^2$ Ajustado

```{r}
k=dim(X)[2]-1
R2.adj = 1-(1-R2)*(n-1)/(n-k-1)

summary(lm(y.bebes~X[,2]+X[,3]+X[,4]+X[,5]))$adj.r.squared
```

### Comparación de modelos

```{r}
summary(lm(y.bebes~X[,2]))$adj.r.squared
summary(lm(y.bebes~X[,2]+X[,3]))$adj.r.squared
summary(lm(y.bebes~X[,2]+X[,3]+X[,4]))$adj.r.squared

AIC(lm(y.bebes~X[,2]))
AIC(lm(y.bebes~X[,2]+X[,3]))
AIC(lm(y.bebes~X[,2]+X[,3]+X[,4]))
AIC(lm(y.bebes~X[,2]+X[,3]+X[,4]+X[,5]))

BIC(lm(y.bebes~X[,2]))
BIC(lm(y.bebes~X[,2]+X[,3]))
BIC(lm(y.bebes~X[,2]+X[,3]+X[,4]))
BIC(lm(y.bebes~X[,2]+X[,3]+X[,4]+X[,5]))
```

### IC para la Regresión Lineal Múltiple

```{r}
lillie.test(errores)

S2 = SSE/(n-k-1)
S2*solve(t(X)%*%X)
errores.estandar = sqrt(S2*diag(solve(t(X)%*%X)))
alpha=0.05

c(estimaciones.b[1]-qt(1-alpha/2,n-k-1)*errores.estandar[1],
  estimaciones.b[1]+qt(1-alpha/2,n-k-1)*errores.estandar[1])

c(estimaciones.b[2]-qt(1-alpha/2,n-k-1)*errores.estandar[2],
  estimaciones.b[2]+qt(1-alpha/2,n-k-1)*errores.estandar[2])

c(estimaciones.b[3]-qt(1-alpha/2,n-k-1)*errores.estandar[3],
  estimaciones.b[3]+qt(1-alpha/2,n-k-1)*errores.estandar[3])

c(estimaciones.b[4]-qt(1-alpha/2,n-k-1)*errores.estandar[4],
  estimaciones.b[4]+qt(1-alpha/2,n-k-1)*errores.estandar[4])

c(estimaciones.b[5]-qt(1-alpha/2,n-k-1)*errores.estandar[5],
  estimaciones.b[5]+qt(1-alpha/2,n-k-1)*errores.estandar[5])

confint(lm(y.bebes~X[,2]+X[,3]+X[,4]+X[,5]),level=0.95)
```

### Para la media

```{r}
alpha=0.05
x0 = c(1,75,50,4,30)
y0.estimado = sum(estimaciones.b*x0)
c(y0.estimado-qt(1-alpha/2,n-k-1)*sqrt(S2*(t(x0)%*%solve(t(X)%*%X)%*%x0)),
  y0.estimado+qt(1-alpha/2,n-k-1)*sqrt(S2*(t(x0)%*%solve(t(X)%*%X)%*%x0)))

# Datos para la función predict.lm
newdata=data.frame(x1=75,x2=50,x3=4,x4=30)
x1=X[,2]
x2=X[,3]
x3=X[,4]
x4=X[,5]

predict.lm(lm(y.bebes~x1+x2+x3+x4),newdata,interval="confidence",level= 0.95)
```

### Para una predicción

```{r}
c(y0.estimado-qt(1-alpha/2,n-k-1)*sqrt(S2*(1+(t(x0)%*%solve(t(X)%*%X)%*%x0))),
  y0.estimado+qt(1-alpha/2,n-k-1)*sqrt(S2*(1+(t(x0)%*%solve(t(X)%*%X)%*%x0))))

predict.lm(lm(y.bebes~x1+x2+x3+x4),newdata,interval="prediction",level= 0.95)
```

### Contrastes de hipótesis sobre los parámetros

```{r}
anova(lm(y.bebes~X[,2:5]))

summary(lm(y.bebes~x1+x2+x3+x4))
```

## Diagnósticos de regresión

- *Errores*: Los errores tienen que seguir una N(0,σ), con la misma varianza, y ser incorrelados.

- *Modelo*: Los puntos se tienen que ajustar a la estructura lineal considerada.

- *Observaciones anómalas*: A veces unas cuántas observaciones no se ajustan al modelo y hay que detectarlas.

### Estudio de los residuos

### Errores:

### Homocedasticidad

```{r}
set.seed(2020)
x<-runif(100)
y<-1-2*x+0.3*x*rnorm(100)
par(mfrow=c(1,2))
plot(x,y)
r=lm(y~x)
abline(r,col="red")
plot(r$res~r$fitted.values,xlab="Valores ajustados",ylab="Residuos del modelo")
```

#### Test de White

El número de variables independientes debe ser menor que el número de valores en la muestra (se suman todas)

```{r}
library(lmtest)

residuos=r$res
X0=length(residuos)*summary(lm(residuos^2~x+I(x^2)))$r.squared
p.valor=pchisq(X0,2,lower.tail = FALSE)

bptest(r,~x+I(x^2))
```

#### Test de Breusch-Pagan

```{r}
coef.deter.modelo.auxiliar = summary(lm(errores^2 ~ X[,2]+X[,3]+X[,4]+X[,5]))$r.squared
X0 = n*coef.deter.modelo.auxiliar
p.valor = pchisq(X0,k,lower.tail=FALSE)

y.bebes=c(57.5,52.8,61.3,67,53.5,62.7,56.2,68.5,69.2)
reg.mul.original = lm(y.bebes~X[,2]+X[,3]+X[,4]+X[,5])
bptest(reg.mul.original)
```

### Normalidad

```{r}
library(car)

estimacion.sigma2 = sum(errores^2)/(n-k-1)
qqPlot(errores,distribution = "norm", mean=0,sd=sqrt(estimacion.sigma2))

lillie.test(errores)
```

### Correlación de los residuos: Test de Durbin-Watson

```{r}
diferencias = errores[2:n]-errores[1:(n-1)]
estadistico.d = sum(diferencias^2)/sum(errores^2)

dwtest(reg.mul.original,alternative = 'greater')
dwtest(reg.mul.original,alternative = 'less')
```

### Modelo:

### Aditividad y linealidad

```{r}
library(car)
```

#### Aditividad

```{r}
valores.ajustados2 = reg.mul.original$fitted.values^2
summary(lm(y.bebes~X[,2]+X[,3]+X[,4]+X[,5]+valores.ajustados2))[[4]]

residualPlots(reg.mul.original,plot=FALSE)
```

#### Linealidad

```{r}
crPlots(reg.mul.original)
```

### Observaciones anómalas:

#### Leverages

```{r}
valores.hat=hatvalues(reg.mul.original)
which(valores.hat > 2*(k+1)/n)
```

#### Outliers

```{r}
#library(car)

outlierTest(reg.mul.original)
```

#### Observaciones influyentes: Distancia de Cook

```{r}
#library(car)

distancias.cook=cooks.distance(reg.mul.original)
which(distancias.cook > 4/(n-k-1))
```

#### Selección del Modelo

```{r}
step(reg.mul.original)
```

## En `Python`

```{python}
import numpy as np
import pandas as pd
import matplotlib 
import matplotlib.pyplot as plt
import scipy.stats as stats
matplotlib.style.use('ggplot')
```

```{python}
mtcars = pd.read_csv("https://raw.githubusercontent.com/MartinSantaGitHub/estadistica-inferencial/master/datasets/mtcars.csv")
```

### Regresión Lineal Simple

```{python}
from sklearn import linear_model

mtcars.plot(kind = "scatter", x = "wt", y = "mpg", figsize = (10,10), color = "black")
plt.show()

regression_model = linear_model.LinearRegression()
regression_model.fit(X = pd.DataFrame(mtcars["wt"]),
                     y = mtcars["mpg"])

regression_model.intercept_
regression_model.coef_

# R^2
regression_model.score(X = pd.DataFrame(mtcars["wt"]), y = mtcars["mpg"])
```

```{python}
train_prediction = regression_model.predict(X = pd.DataFrame(mtcars["wt"]))
train_prediction

residuals = mtcars["mpg"] - train_prediction
residuals
residuals.describe()

SSE = (residuals**2).sum()
SST = ((mtcars["mpg"]- mtcars["mpg"].mean())**2).sum()
1-(SSE/SST)
```

```{python}
mtcars.plot(kind = "scatter", x = "wt", y = "mpg", figsize = (10,10), color = "black", xlim = (0, 7))
plt.plot(mtcars["wt"], train_prediction, color = "red")
plt.show()
```

### Análisis de los residuos

#### RMSE

```{python}
from sklearn.metrics import mean_squared_error

plt.figure(figsize=(10,10))

# Estudio de la normalidad de los residuos
stats.probplot(residuals, dist="norm", plot = plt)
RMSE = mean_squared_error(train_prediction, mtcars["mpg"])**0.5
RMSE
```

### Outliers

```{python}
mtcars_subset = mtcars[["mpg", "wt"]]
supercar = pd.DataFrame({"mpg":50, "wt": 10}, index = ["super"])
new_cars = mtcars_subset.append(supercar)

#regression_model = linear_model.LinearRegression()
regression_model.fit(X = pd.DataFrame(new_cars["wt"]),
                     y = new_cars["mpg"])
                     
regression_model.score(X = pd.DataFrame(new_cars["wt"]), y = new_cars["mpg"])    
```

```{python}
train_prediction2 = regression_model.predict(X = pd.DataFrame(new_cars["wt"]))

new_cars.plot(kind="scatter", x = "wt", y = "mpg", figsize = (10,10), color = "black", xlim = (0, 11), ylim = (10, 52))
plt.plot(new_cars["wt"], train_prediction2, color = "red")
```

### Regresión polinómica

```{python}
poly_model = linear_model.LinearRegression()
predictors = pd.DataFrame([mtcars["wt"], mtcars["wt"]**2]).T
poly_model.fit(X = predictors, y = mtcars["mpg"])

poly_model.intercept_
poly_model.coef_

poly_model.score(X = predictors, y = mtcars["mpg"])
```

```{python}
poly_range = np.arange(0, 6, 0.1)
poly_regr = pd.DataFrame([poly_range, poly_range**2]).T
y_regr = poly_model.predict(X = poly_regr)

mtcars.plot(kind="scatter", x = "wt", y = "mpg", figsize = (10,10), color = "black", xlim = (0,6))
plt.plot(poly_range, y_regr, color = "red")
plt.show()
```

#### RMSE

```{python}
#from sklearn.metrics import mean_squared_error

preds = poly_model.predict(X = predictors)
mean_squared_error(preds, mtcars["mpg"])**0.5
```

### Overfitting

#### Modelo polinómico de orden superior

```{python}
poly_model = linear_model.LinearRegression()

predictors = pd.DataFrame([mtcars["wt"], mtcars["wt"]**2, mtcars["wt"]**3, mtcars["wt"]**4, mtcars["wt"]**5,
                           mtcars["wt"]**6, mtcars["wt"]**7, mtcars["wt"]**8, mtcars["wt"]**9, mtcars["wt"]**10]).T

poly_model.fit(X = predictors, y = mtcars["mpg"])
```

```{python}
poly_model.intercept_
poly_model.coef_

poly_model.score(X = predictors, y = mtcars["mpg"])
```

```{python}
x_range = np.arange(1.5, 5.45, 0.01)
poly_reg = pd.DataFrame([x_range, x_range**2, x_range**3, x_range**4, x_range**5, x_range**6, x_range**7, x_range**8, x_range**9, x_range**10]).T
y_pred = poly_model.predict(X = poly_reg)

mtcars.plot(kind = "scatter", x="wt", y="mpg", figsize = (10,10), color = "black", xlim = (0, 7))
plt.plot(x_range, y_pred, color = "red")
plt.show()
```

```{python}
#from sklearn.metrics import mean_squared_error

preds = poly_model.predict(X = predictors)
mean_squared_error(preds, mtcars["mpg"])**0.5
```

### Regresión Lineal Múltiple

```{python}
multi_reg_model = linear_model.LinearRegression()
multi_reg_model.fit(X = mtcars.loc[:, ["wt", "hp"]],
                    y = mtcars["mpg"])
                    
multi_reg_model.intercept_
multi_reg_model.coef_

multi_reg_model.score(X = mtcars.loc[:, ["wt", "hp"]], y = mtcars["mpg"])
```

```{python}
mtcars.plot(kind = "scatter", x="hp", y = "mpg", figsize=(10,10), color = "black")
plt.show()
```

```{python}
preds = multi_reg_model.predict(X = mtcars.loc[:, ["wt", "hp"]])
mean_squared_error(preds, mtcars["mpg"])**0.5
```

```{python}
multi_reg_model = linear_model.LinearRegression()

poly_reg = pd.DataFrame([mtcars["wt"], mtcars["wt"]**2, mtcars["hp"], mtcars["hp"]**2]).T

multi_reg_model.fit(X = poly_reg, y = mtcars["mpg"])

multi_reg_model.score(X = poly_reg, y = mtcars["mpg"])

mean_squared_error(multi_reg_model.predict(X = poly_reg), mtcars["mpg"])**0.5
```

#### Cuestionario 10:

```{r}
library(lmtest)
library(fBasics)
library(car)

reg.lineal.simple = lm(mpg~wt,data=mtcars)
summary(reg.lineal.simple)
plot(mpg~wt,data=mtcars,col='blue',pch=16)
abline(reg.lineal.simple,col='red')

#confint(lm(mpg~wt,data=mtcars),level=0.90)

predict.lm(reg.lineal.simple,data.frame(wt=4),interval="confidence",level= 0.90)

reg.lineal.multiple = lm(mpg~wt+hp+qsec,data=mtcars)
summary(reg.lineal.multiple)
plot(mpg~hp,data=mtcars,col='blue',pch=16)
abline(lm(mpg~hp,data=mtcars),col='red')
plot(mpg~qsec,data=mtcars,col='blue',pch=16)
abline(lm(mpg~qsec,data=mtcars),col='red')

# Comparación de modelos
step(reg.lineal.multiple)

# Test de Breusch-Pagan: Homocedasticidad
bptest(reg.lineal.multiple)

# Normalidad
dagoTest(reg.lineal.multiple$residuals)
estimacion.sigma2 = sum(reg.lineal.multiple$residuals^2)/(length(reg.lineal.multiple$residuals)-3-1)
qqPlot(reg.lineal.multiple$residuals,distribution = "norm", mean=0,sd=sqrt(estimacion.sigma2))

# Correlación
dwtest(reg.lineal.multiple,alternative = 'greater')
#dwtest(reg.lineal.multiple,alternative = 'two.sided')
dwtest(reg.lineal.multiple,alternative = 'less')

# Aditividad: Test de Tukey
residualPlots(reg.lineal.multiple,plot=FALSE)

# Linealidad
crPlots(reg.lineal.multiple)

# Leverages
k=3
n=length(mtcars[,"mpg"])
valores.hat=hatvalues(reg.lineal.multiple)
which(valores.hat > 2*(k+1)/n)

# Outliers
outlierTest(reg.lineal.multiple)

# Observaciones influyentes: Distancia de Cook
distancias.cook=cooks.distance(reg.lineal.multiple)
which(distancias.cook > 4/(n-k-1))
```
